{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "t5_tifu_summary.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkg3gMNw7K1EjokUW7jQmt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ACM-Research/NLP-Summarizer/blob/master/Notebooks/t5-summarizer/t5_tifu_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF6iYS14j2Fx"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCBpqOvGta6h",
        "outputId": "c922d3f0-4750-44be-a12f-f825433a4dc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJT1vo7tsdmp",
        "outputId": "9c7c13f8-e7b0-4e3d-9bfb-c550f7c17c3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import json \n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
        "import os,sys\n",
        "import json\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "#T5 model Types\n",
        "#\n",
        "#t5-small\n",
        "#t5-base\n",
        "#t5-large\n",
        "#t5-3B\n",
        "#t5-11B\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "#load Spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Configures model to use GPU\n",
        "device = torch.device('cuda')\n",
        "model.to(torch.device(\"cuda:0\"))\n",
        "\n",
        "print('download complete')\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "download complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c0X3A4HkVc3"
      },
      "source": [
        "path = \"/content/drive/My Drive/tifu_all_tokenized_and_filtered.json\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7PhFGXtqgEh"
      },
      "source": [
        "os.mkdir(\"/content/drive/My Drive/tifuSummaries\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6ACXQR_mZ4p"
      },
      "source": [
        "data = []\n",
        "with open(path) as file:\n",
        "  for line in file:\n",
        "    data.append(json.loads(line))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g4X2reXkTMO"
      },
      "source": [
        "  summaries = open(\"/content/drive/My Drive/tifuSummaries/tifu_summaries.json\",\"w\")\n",
        "\n",
        "  for index,tifu in enumerate(data):\n",
        "    if(index <= 100):\n",
        "      jsonStuff = {}\n",
        "      \n",
        "      text = tifu[\"selftext_without_tldr\"]\n",
        "\n",
        "    # Adjust text stripper to make sure text is stripped properly\n",
        "      preprocess_text = text.strip().replace(\"\\n\",\" \")\n",
        "      t5_prepared_Text = \"summarize: \"+preprocess_text\n",
        "      print (\"original text preprocessed: \\n\", preprocess_text)\n",
        "\n",
        "      # Adjust max_length for speech size based on tokens\n",
        "      tokenized_text = tokenizer.encode(t5_prepared_Text,truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "\n",
        "      # summmarize \n",
        "      # Change no_repeat_ngram_size to adjust size of Ngrams in summary\n",
        "      summary_ids = model.generate(tokenized_text,\n",
        "                                          num_beams=4,\n",
        "                                          no_repeat_ngram_size=8,\n",
        "                                          min_length=30,\n",
        "                                          max_length=100,\n",
        "                                          early_stopping=True)\n",
        "\n",
        "      output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "      print (\"\\nSummarized text: \\n\",output)\n",
        "      print()\n",
        "\n",
        "      original_text_pos = nlp(preprocess_text)\n",
        "      summary_pos = nlp(output)\n",
        "\n",
        "      print()\n",
        "\n",
        "      jsonStuff = {\"original_text:\": preprocess_text, \n",
        "                  \"summary_text\": output,\n",
        "                  \"compression_ratio\": str(len(output)/len(preprocess_text)),\n",
        "                  \"original_pos_tags\": Counter([token.pos_ for token in original_text_pos]),\n",
        "                  \"summary_pos_tags\": Counter([token.pos_ for token in summary_pos])\n",
        "                   }\n",
        "      \n",
        "      summaries.write(json.dumps(jsonStuff) + \"\\n\")\n",
        "      print(\"written\\n\")\n",
        "\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  summaries.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}